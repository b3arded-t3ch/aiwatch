insert into organization_documents(url, title, publication_date, modified_date, author, publisher, affected_organizations, affected_people, document_scope, cause_area, notes) values
   (
        'https://www.reddit.com/r/MachineLearning/comments/5zgbyy/d_what_is_the_job_interview_process_like_at_openai/', /* url */
        '[D] What is the job interview process like at OpenAI?', /* title */
        '2017-03-14', /* publication_date */
        '2017-03-14', /* modified_date */
        'zergylord', /* author */
        'Reddit', /* publisher */
        'OpenAI', /* affected_organizations */
        NULL, /* affected_people */
        'Job application experience', /* document_scope */
        'AI', /* cause_area */
        'The discussion thread includes descriptions of interview experiences by a few people, including one who went through the process but was rejected and one who went through the process, was accepted, but ultimately didn''t join. One reply suggests that open source contributions may help with landing an on-site interview' /* notes */
   )
  ,(
        'https://www.quora.com/What-was-your-experience-like-as-an-intern-at-OpenAI', /* url */
        'What was your experience like as an intern at OpenAI?', /* title */
        '2018-07-16', /* publication_date */
        '2018-07-19', /* modified_date */
        NULL, /* author */
        'Quora', /* publisher */
        'OpenAI', /* affected_organizations */
        'Kevin Frans', /* affected_people */
        'Job experience', /* document_scope */
        'AI', /* cause_area */
        'The question asks about people''s internship experience at OpenAI. There is one answer by Kevin Frans, who describes his internship experience from 2017. His first two weeks involved picking a project idea, after which his time was spent mainly on execution. He likes the open and friendly atmosphere, and the fact that people usually eat lunch together and can strike up conversations during lunch' /* notes */
   )
  ,(
        'https://www.reddit.com/r/MachineLearning/comments/63q056/d_what_do_interns_do_at_openai/', /* url */
        '[D] What do interns do at OpenAI?', /* title */
        '2017-04-05', /* publication_date */
        '2017-04-06', /* modified_date */
        'gsjbjt', /* author */
        'Reddit', /* publisher */
        'OpenAI', /* affected_organizations */
        'Ilya Sutskever', /* affected_people */
        'Job experience', /* document_scope */
        'AI', /* cause_area */
        'There is one substantive answer from Ilya Sutskever (Research Director), who says that interns have significant freedom, get to work closely with researchers, and that OpenAI generally hires non-PhD interns' /* notes */
   )
  ,(
        'https://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/', /* url */
        'AMA: the OpenAI Research Team', /* title */
        '2016-01-08', /* publication_date */
        '2016-01-09', /* modified_date */
        'Ilya Sutskever', /* author */
        'Reddit', /* publisher */
        'OpenAI', /* affected_organizations */
        'Ilya Sutskever|Andrej Karpathy|Durk Kingma|Greg Brockman|John Schulman|Vicki Cheung|Wojciech Zaremba', /* affected_people */
        'Job experience', /* document_scope */
        'AI', /* cause_area */
        '6 members of the OpenAI research team, including Research Director Ilya Sutskever, conduct an Ask Me Anything (AMA) on Reddit. This is about one month after the official launch of OpenAI' /* notes */
   )
  ,(
        'https://80000hours.org/podcast/episodes/the-world-needs-ai-researchers-heres-
	how-to-become-one/', /* url */
        'How to train for a job developing AI at OpenAI or DeepMind', /* title */
        '2017-07-21', /* publication_date */
        NULL, /* modified_date */
        'Robert Wiblin', /* author */
        '80,000 Hours', /* publisher */
        'OpenAI|DeepMind', /* affected_organizations */
        'Robert Wiblin|Daio Amodei', /* affected_people */
        'Job experience', /* document_scope */
        'AI', /* cause_area */
        'Robert Wiblin interviews Dario Amodei for the 80,000 Hours podcast about working at OpenAI and about the domains of AI and AI safety. The latter half of the podcast includes advice for people training to work in AI organizations such as OpenAI and DeepMind' /* notes */
   )
  ,(
        'https://www.lesswrong.com/posts/dJQo7xPn4TyGnKgeC/hiring-engineers-and-
	researchers-to-help-align-gpt-3', /* url */
        'Hiring engineers and researchers to help align GPT-3', /* title */
        '2020-10-01', /* publication_date */
        NULL, /* modified_date */
        'Paul Christiano', /* author */
        'LessWrong', /* publisher */
        'OpenAI', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'Paul Christiano posts on LessWrong a hiring note asking for engineers and researchers to work on GPT-3 alignment problems, as the language model is already being deployed in the OpenAI API' /* notes */
   )
,(
	'https://x.com/janleike/status/1352681093007200256', /* url */
	'Last week I joined OpenAI to lead their alignment effort.', /* title */
	'2021-01-22', /* publication_date */
	NULL, /* modified_date */
	'Jan Leike', /* author */
	'Twitter', /* publisher */
	'OpenAI', /* affected_organizations */
	'Jan Leike', /* affected_people */
	'Successful hire', /* document_scope */
	'AI Safety', /* cause_area */
	'In a tweet, Jan Leike announces joining OpenAI to lead the alignment effort. In comment, he states that his reason for joining OpenAI is that he loves OpenAI''s work on reward modeling and aligning GPT-3 using human preferences, and that he looks forward to building on it. However, in a 2024 tweet, Leike would reveal he joined OpenAI as he had thought it would be a better place to do research.' /* notes */
)
,(
	'https://openai.com/index/discovering-the-minutiae-of-backend-systems/', /* url */
	'Discovering the minutiae of backend systems', /* title */
	'2022-08-22', /* publication_date */
	NULL, /* modified_date */
	'OpenAI', /* author */
	'OpenAI', /* publisher */
	'OpenAI', /* affected_organizations */
	'Christian Gibson', /* affected_people */
	'Job experience', /* document_scope */
	'AI', /* cause_area */
	'This article contains an interview with Christian Gibson, an engineer on the supercomputing team of OpenAI, discussing how he got into programming as well as his work at OpenAI. In the interview, he discusses that his work focuses on solving problems that relate to Exploratory AI workflows by preempting research needs before they block progress and identifying bottlenecks as well as implementing workarounds as quickly as possible. Gibson''s account of working on supercomputing at OpenAI provides insights into the organization''s operations and culture. According to Gibson, OpenAI operates at an unprecedented scale, utilizing billion-dollar supercomputers and encountering technical challenges that push the boundaries of existing hardware capabilities. He mentions that the employees actively work to save days of compute time by relentlessly focusing on performance optimization to enable them match the cutting-edge technology. The work environment is characterized by a mix of coding, problem-solving, and collaboration, with employees having a clear understanding of how their work impacts specific teams and projects. According to him, employees also have the opportunity to tackle unique technical challenges in high-performance computing which contributes to employee motivation.' /* notes */
)
,(
	'https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-
	society-acknowledges/story?id=97897122', /* url */
	'OpenAI CEO Sam Altman says AI will reshape society,
	acknowledges risks: ''A little bit scared of this''', /* title */
	'2023-03-16', /* publication_date */
	NULL, /* modified_date */
	'Victor Ordonez|Taylor Dunn|Eric Noll', /* author */
	'abc NEWS', /* publisher */
	'OpenAI', /* affected_organizations */
	'Sam Altman', /* affected_people */
	'General discussion of organizational practices', /* document_scope */
	'AI Safety', /* cause_area */
	'A few months after the release of GPT-4, ABC News'' chief business, technology and economics correspondent Rebecca Jarvis interview Sam Altman about the rollout of GPT-4. In the interview, Sam Altman expresses his concerns about potential misuse of the AI technology; however, he maintains that OpenAI is in regular contact with government officials as the organization needs both regulators and society to prevent the potential negative consequences the technology could have on human.' /* notes */
)
,(
	'https://time.com/6288245/openai-eu-lobbying-ai-act/',  /* url */
	'Exclusive: OpenAI Lobbied the E.U. to Water Down AI Regulation',  /* title */
	'2023-06-20', /* publication_date */
	NULL, /* modified_date */
	'Billy Perrigo', /* author */
	'TIME', /* publisher */
	'OpenAI', /* affected_organizations */ 
	'Sam Altman', /* affected_people */
	'Third-party commentary on organization', /* document_scope */
	'AI Safety', /* cause_area */
	'The author of this article claims that OpenAI manages to influence the E.U.''s AI Act. He argues that although Sam Altman has been the major preacher for global AI regulation, he lobbies, behind the scenes, for certain elements of the legislation to be in the favour of OpenAI. According to him, "OpenAI’s lobbying effort appears to have been a success: the final draft of the Act approved by E.U. lawmakers did not contain wording present in earlier drafts suggesting that general purpose AI systems should be considered inherently high risk". Meanwhile, the author claims that "in 2022, OpenAI repeatedly argued to European officials that the forthcoming AI Act should not consider its general purpose AI systems—including GPT-3, the precursor to ChatGPT, and the image generator Dall-E 2—to be high risk,"' /* notes */
)
,(
	'https://openai.com/index/openai-announces-leadership-transition/', /* url */
        'OpenAI announces leadership transition', /* title */
        '2023-11-17', /* publication_date */
        NULL, /* modified_date */
        'OpenAI', /* author */
        'OpenAI', /* publisher */
        'OpenAI', /* affected_organizations */
        'Sam Altman', /* affected_people */
        'Employee departure', /* document_scope */
        'AI Safety', /* cause_area */
        'This article highlights the decision of the the board of directors of OpenAI to let relieve Sam Altman of his position as the CEO of the organization. According to them, his dismissal hinges on gaps in communications with the board which hinder its ability to exercise its responsibilities. However, a tweet mentions that Sam''s dismissal was sequel to some internal disagreements about safe development of AI.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/eHFo7nwLYDzpuamRM/sam-altman-fired-from-
	openai#SYhfi45LhtX75Tr9v', /* url */
        'Hmmm. The way Sam behaves I can''t see a path of him leading an AI company towards safety', /* title */
        '2023-11-18', /* publication_date */
        NULL, /* modified_date */
        'MiguelDev', /* author */
        'LessWrong',  /* publisher */
        'OpenAI', /* affected_organizations */
        'Sam Altman', /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI Safety', /* cause_area */
	'MiguelDev, in a comment, describes what a typical CEO of OpenAI should be like. He says, "A CEO I wish OpenAI has - is someone who stays at the offices, ensuring that we are on track of safely steering arguably the most revolutionary tech ever created - not trying to promote the company or the tech, I think it''s unnecessary to do a world tour if one is doing AI development and deployment safely."' /* notes */
)
,(
	'https://www.lesswrong.com/posts/gZkYvA6suQJthvj4E/my-may-2023-priorities-for-
	ai-x-safety-more-empathy-more4', /* url */
	'My May 2023 priorities for AI x-safety: more empathy,
	more unification of concerns, and less vilification of OpenAI', /* title */
        '2023-05-24', /* publication_date */
	NULL, /* modified_date */
	'Andrew_Critch', /* author */
	'LessWrong', /* publisher */
        'OpenAI', /* affected_organizations */
        NULL, /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI|AI Safety', /* cause_area */
	'In this post, Andrew_Critch expresses his general perspective about OpenAI. He mentions reasons for his positive views of OpenAI like transparency, charter effectiveness, public engagement, etc. He thus calls for balanced critique of the organization saying that harsh criticism of OpenAI might be counterproductive for overall AI safety.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/jfYnq8pKLpKLwaRGN/transcript-yudkowsky-on-
	bankless-follow-up-q-and-a', /* url */
	'Transcript: Yudkowsky on Bankless follow-up Q&A', /* title */
        '2023-02-28', /* publication_date */
	NULL, /* modified_date */
	NULL, /* author */
        'LessWrong', /* publisher */
        'OpenAI', /* affected_organizations */
        'Eliezer', /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI Safety', /* cause_area */
        'In an interview, Eliezer Yudkowsky expresses a critical view of OpenAI saying that OpenAI''s efforts in AI safety is insufficient and ineffective. He argues that their actions are setting bad examples and intensifying competition in AI development, which he sees as dangerous. If given control, Yudkowsky says he would dramatically change OpenAI''s approach. He would rename it to "ClosedAI," cut ties with Microsoft, stop generating hype, and focus on developing more alignable AI systems rather than just more powerful ones.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/3S4nyoNEEuvNsbXt8/common-misconceptions-about-openai', /* url */
	'Common misconceptions about OpenAI', /* title */
	'2022-08-25', /* publication_date */
        NULL, /* modified_date */
	'Jacob_Hilton', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
        NULL, /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI Safety', /* cause_area */
        'This post contains the author''s list of common misconceptions about OpenAI. He lists and explains some accurate impressions, common misconceptions and his personal views about OpenAI.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/cy99dCEiLyxDrMHBi/what-s-going-on-with-openai-s-messaging', /* url */
	'What''s Going on With OpenAI''s Messaging?', /* title */
	'2024-05-21', /* publication_date */
	NULL, /* modified_date */
	'ozziegooen', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	'Sam Altman', /* affected_people */
	'Third-party commentary on organization ', /* document_scope */
	'AI Safety', /* cause_area */
	'In this post, the author expresses his criticism against OpenAI operations saying that its actions are not different from those of traditional high-growth tech startups. These criticism as he explains hinge on contradictory messaging, safety concerns, incoherent promises amongst others.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/Nqn2tkAHbejXTDKuW/openai-makes-humanity-less-safe', /* url */
	'OpenAI makes humanity less safe',  /* title */
	'2017-04-03', /* publication_date */
	NULL, /* modified_date */
	'Benquo', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	NULL, /* affected_people */
	'Third-party commentary on organization', /* document_scope */
	'AI Safety', /* cause_area */
	'According to Benquo, OpenAI is the result of the efforts of some "good people" to address AI safety. However, Benquo expresses confusion as to how well-intentioned efforts to address AI safety have resulted in an organization that may be exacerbating the very problems it aims to solve. He argues that despite the good intentions behind it, OpenAI may be increasing the risks associated with superintelligent.' /* notes */
)
,(
	'https://www.bloomberg.com/news/articles/2024-05-28/ex-openai-director-says-board-
	learned-of-chatgpt-launch-on-twitter', /* url */
	'Ex-OpenAI Director Says Board Learned of ChatGPT Launch on Twitter', /* title */
	'2024-05-28', /* publication_date */
	NULL, /* modified_date */
	'Shirin Ghaffary', /* author */
	'Bloomberg', /* publisher */
	'OpenAI', /* affected_organizations */
	'Sam Altman', /* affected_people */
	'Third-party commentary on organization', /* document_scope */
	'AI Safety', /* cause_area */
	'Helen Toner, a former OpenAI board member, reveals a "did-you-know" in a podcast called The TED AI Show saying that "When ChatGPT came out in November 2022, the board was not informed in advance about that, we learned about ChatGPT on Twitter."' /* notes */
)
,(
	'https://www.lesswrong.com/posts/dd66GymgbLQMHGLwQ/openai-helen-toner-speaks', /* url */
	'OpenAI: Helen Toner Speaks', /* title */
	'2024-05-30', /* publication_date */
	NULL, /* modified_date */
	'Zvi', /* author */
	'LessWrong',  /* publisher */
	'OpenAI', /* affected_organizations */
	'Sam Altman', /* affected_people */
	'Third-party commentary on organization', /* document_scope */
	'AI Safety|AI', /* cause_area */
	'This post contains the author''s notes on the podcast of Helen Toner with TED AI. It contains an account of governance challenges faced by OpenAI under the leadership of Sam Altman. It suggests that under Sam Altman''s leadership, OpenAI struggled with transparency and accountability issues, including alleged instances of withholding information and misrepresentation to the board. He expresses concerns that even well-intentioned organizations can be negatively influenced due to misaligned incentives and governance failures which can make them compromise their original goals of ensuring AI safety and benefiting humanity.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/q3zs7E7rktHsESXaF/openai-8-the-right-to-warn', /* url */
	'OpenAI #8: The Right to Warn', /* title */
	'2024-06-17', /* publication_date */
	NULL, /* modified_date */
	'Zvy',  /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	'Leopold Aschenbrenner', /* affected_people */
	'Third-party commentary on organization', /* document_scope */
	'AI Safety', /* cause_area */
	'Zvy reveals the details surrounding Leopold''s firing. According to him, Leopold''s refusal to sign a letter demanding the board''s resignation, writing a memo about OpenAI''s cybersecurity issues and sharing it with the board as well as retaliation for whistleblowing and not showing sufficient loyalty to Sam Altman.'  /* notes */
)
,(
	'https://www.lesswrong.com/posts/sXhBCDLJPEjadwHBM/boycott-openai', /* url */
	'Boycott OpenAI', /* title */
	'2024-06-18', /* publication_date */
	NULL, /* modified_date */
	'PeterMcCluskey', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	'Sam Altman', /* affected_people */
	'Third-party commentary on organization ', /* document_scope */
	'AI Safety|AI', /* cause_area */
	'This post reflects lack of trust in ethics and transparency in the works of OpenAI. For these reasons, the author discusses his decision to boycott OpenAI.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/YwhgHwjaBDmjgswqZ/openai-fallout', /* url */
	'OpenAI: Fallout',  /* title */
	'2024-05-28', /* publication_date */
	NULL, /* modified_date */
	'Zvi', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	'Kelsey Piper|Daniel Kokotajlo', /* affected_people */
	'HR Controversy', /* document_scope */
	'AI Safety', /* cause_area */
	'This post reveals controversies on how OpenAI treats her current and former employees particularly as it relates to off-boarding agreements and equity disputes. The author exposes OpenAI''s threat to confiscate vested equity from any departing member except such a member signs a non-disparagement agreements. According to the author, this issue was not known until Daniel Kokotajlo refused to sign which led to reporting by Kelsey Piper.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/2z5vrsu7BoiWckLby/an-openai-board-seat-is-surprisingly-expensive',  /* url */
	'An OpenAI board seat is surprisingly expensive', /* title */
	'2017-04-19', /* publication_date */
	NULL, /* modified_date */
	'Benquo', /* author */
	'LessWrong',  /* publisher */
	'OpenAI|Open Philanthropy', /* affected_organizations */
	NULL, /* affected_people */
	'Third-party commentary on organization', /* document_scope */
	'AI Safety', /* cause_area */
	'According to the author, a $30 million donation for a board seat in OpenAI is "surprisingly expensive" for OpenPhil. The author insinuates that this deal is more about status, positioning and social signaling in the AI safetfy space; he thinks the amount of the donation was strategically chosen to undermine other AI safetfy organizations that are supported by OpenPhil.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/yRWv5kkDD4YhzwRLq/non-disparagement-canaries-for-openai', /* url */
	'Non-Disparagement Canaries for OpenAI', /* title */
	'2024-05-30', /* publication_date */
	NULL, /* modified_date */
	'aysja|Adam Scholl', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	NULL, /* affected_people */
	'Organization operations', /* document_scope */
	'AI Safety', /* cause_area */
	'According to the authors of this post, OpenAI has a practice of non-disparagement agreements for departing employees. While recognizing AI Watch for making the data available, they estimate that over 500 former employees of OpenAI may have signed these agreements where only just 5 publicly reported to have been released and 7 people who have publicly reported not being subject to the term so far. Also, they hold that these agreements allow OpenAI to systematically silence criticism from its former employees.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/JSWF2ZLt6YahyAauE/ilya-sutskever-and-jan-leike-resign-from-openai-updated#M62wLnAKb3FdFdoF5', /* url */
	'Ilya Sutskever and Jan Leike resign from OpenAI', /* title */
	'2024-05-15', /* publication_date */
	NULL, /* modified_date */
	'Zach Stein-Perlman', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	'Ilya Sutskever|Jan Leike|Jakub Pachocki', /* affected_people */
	'Employee Departure', /* document_scope */
	'AI Safety', /* cause_area */
	'This post discusses the resignation of two key figures at OpenAI, Ilya Sutskever and Jan Leike, stating the events that surround their departures. However, in a comment https://www.lesswrong.com/posts/JSWF2ZLt6YahyAauE/ilya-sutskever-and-jan-leike-resign-from-openai-updated?commentId=M62wLnAKb3FdFdoF5, Thane Ruthenis expresses pleasure at the resignations at OpenAI saying that it is a clear indication of safety-washing and virtue signaling by the organization and that the resignations signal that OpenAI doesn''t deeply care about AI safety.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/N8aRDYLuakmLezeJy/do-not-mess-with-scarlett-johansson', /* url */
	'Do Not Mess With Scarlett Johansson', /* title */
	'2024-05-22',  /* publication_date */
	NULL, /* modified_date */
	'Zvi', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	'Scarlett Johansson|Sam Altman', /* affected_people */
	'Third-party commentary on organization ', /* document_scope */
	'AI Safety|AI', /* cause_area */
	'After Johansson had declined OpenAI''s proposal for her to be the voice of the AI, the author claims that OpenAI proceeded with a voice quite similar to her own. This, thus, raises concerns about transparency, consent and trustworthiness of OpenAI and AI companies in general. In furtherance, the author examines whether Johansson has a case or not saying that if she has no case, it could signal no protections against voice imitation which can be dangerous for public figures.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/vgCoy4bBrDw9LPrpW/what-do-we-know-about-the-ai-knowledge-and-views-especially',  /* url */
	'What do we know about the AI knowledge and views, especially about existential risk, of the new OpenAI board members?', /* title */
	'2024-03-11',  /* publication_date */
	NULL, /* modified_date */
	'Zvi', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	'Dr. Sue Desmond-Hellmann|Nicole Seligman|Fidji Simo|',  /* affected_people */
	'General discussion of organizational practices', /* document_scope */
	'AI Safety', /* cause_area */
	'Sequel to the announcement https://openai.com/blog/openai-announces-new-members-to-board-of-directors of new board members to oversee activities at OpenAI, the author seeks information as per "their views or knowledge on any AI-related subjects". He says, no matter how little, there should be a piece of information on this.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/ANStTHjj6it8ysaRJ/why-did-openai-employees-sign', /* url */
	'why did OpenAI employees sign', /* title */
	'2023-11-27', /* publication_date */
	NULL, /* modified_date */
	'bhauth', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	'Sam Altman', /* affected_people */
	'General discussion of organizational practices', /* document_scope */
	'AI|AI Safety', /* cause_area */
	'The author offers suggestions on what might be the reasons for the employees'' behaviour that made them to sign an open letter https://www.nytimes.com/interactive/2023/11/20/technology/letter-to-the-open-ai-board.html which demands the reinstatement of Sam Altman as the company''s CEO.' /* notes */
)
,(
	'https://www.theinformation.com/articles/trio-of-leaders-leave-openai', /* url */
	'Trio of OpenAI Leaders Depart, Take Leave of Absence', /* title */
	'2024-08-05', /* publication_date */
	NULL, /* modified_date */
	'Stephanie Palazzolo|Jon Victor|Amir Efrati', /* author */
	'The Information', /* publisher */
	'OpenAI', /* affected_organizations */
	'Greg Brockman|John Schulman|Peter Deng', /* affected_people */
	'Employee Departure', /* document_scope */
	'AI Safety', /* cause_area */
	'The post explains that there are some shake-ups within OpenAI. Although the authors provides no reason for the departure of some of the members of OpenAI, they do hint that the company''s "leadership has yet to be stabilized after firing and rehiring Sam Altman". However, John Schulman explains https://x.com/johnschulman2/status/1820610863499509855 that his departure from OpenAI is not hinged on "lack  of support for alignment research at OpenAI", but that it''s a personal decision "to pursue this goal at Anthropic, where I believe I can gain new perspectives and do research alongside people deeply engaged with the topics" he finds more interesting.' /* notes */
)
,(
	'https://openai.com/index/openai-appoints-retired-us-army-general/', /* url */
	'OpenAI appoints Retired U.S. Army General Paul M. Nakasone to Board of Directors', /* title */
	'2024-06-13', /* publication_date */
	NULL, /* modified_date */
	'OpenAI', /* author */
	'OpenAI', /* publisher */
	'OpenAI', /* affected_organizations */
	'Paul M. Nakasone', /* affected_people */
	'Successful hire', /* document_scope */
	'AI Safety', /* cause_area */
	'OpenAI announces the appointment of Retired U.S. Army General Paul M. Nakasone as a new member of its Board of Directors.' /* notes */
)
,(
	'https://www.lesswrong.com/posts/eZxG2E4B44RyTFGpE/openai-appoints-retired-u-s-army-general-paul-m-nakasone-to?commentId=4fX2w72sttyC8TnqB', /* url */
	'This fact will be especially important insofar as a situation arises where e.g. some engineers at the company think that the latest system isn''t safe.', /* title */
	'2024-06-14', /* publication_date */
	NULL, /* modified_date */
	'Daniel Kokotajlo', /* author */
	'LessWrong', /* publisher */
	'OpenAI', /* affected_organizations */
	NULL, /* affected_people */
	'Third-party commentary on organization', /* document_scope */
	'AI Safety', /* cause_area */
	'As a reply to a comment https://www.lesswrong.com/posts/eZxG2E4B44RyTFGpE/openai-appoints-retired-u-s-army-general-paul-m-nakasone-to?commentId=5GDL4PT6iwt4Bk3DN on the appoint of Retired U.S. Army General Paul M. Nakasone, Daniel Kokotajlo highlights a potential critical governance issue that may arise if the board consists of people with no technical expertise. According to Daniel, they may not be able address or evaluate safety concerns raised by engineers which may lead to decisions based on deference rather than understanding.' /* notes */
)
,(
	'https://www.ted.com/talks/the_ted_ai_show_what_really_went_down_at_openai_and_the_future_of_regulation_w_helen_toner?subtitle=en', /* url */
	'What really went down at OpenAI and the future of regulation w/ Helen Toner', /* title */
	'2024-05-28', /* publication_date */
	NULL, /* modified_date */
	'The TEDAI Show', /* author */
	'TED', /* publisher */
	'OpenAI', /* affected_organizations */
	'Sam Altman|Helen Toner', /* affected_people */
	'Third-party commentary on organization', /* document_scope */
	'AI Safety', /* cause_area */
	'In this interview, Helen Toner discusses the events that surround the ousting of Sam Altman in late 2023. Amongst other reasons, she mentions that Sam Altman was standing in the way of the board in carrying out its oversight function as he withheld information from them, and he lied in some cases.' /* notes */
);
